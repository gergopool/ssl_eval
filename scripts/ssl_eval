#!/usr/bin/python3

# Credits: https://github.com/facebookresearch/simsiam/blob/main/main_lincls.py
import argparse
import os
import random
import torch
import torch.backends.cudnn as cudnn
import torch.multiprocessing as mp
from torchvision import models

from typing import List, Union

import ssl_eval


def run_multi_gpu(args):
    if args.devices:
        str_devices = ','.join(args.devices)
        os.environ['CUDA_VISIBLE_DEVICES'] = str_devices
    num_gpus = torch.cuda.device_count()

    if args.seed:
        set_random_seed(args.seed)
    port = random.randint(0, 9999) + 40000  # random port

    if not isinstance(method, list):
        method = [method]

    for m in args.method:
        if m not in ["linear_eval", "knn", "snn", ""]:
            raise print(f"WARNING: Method {m} is unknown and therefore will be ignored.")

    result_dict = mp.Manager().dict() if num_gpus > 1 else {}
    if len(num_gpus) > 1:
        mp.spawn(process, nprocs=num_gpus, args=(num_gpus, args, result_dict))
    else:
        process(0, 1, args, result_dict)

    return result_dict


def process(rank, world_size, args, results):

    if torch.cuda.is_available():
        device = torch.cuda.device(rank)
        torch.cuda.set_device(device)

    ssl_eval.distributed.init_distributed(args.port, rank_and_world_size=(rank, world_size))
    if world_size > 1:
        print(f"Rank{rank} started succesfully.")
        torch.distributed.barrier()

    state_dict = torch.load(args.model_path, map_location=device)
    model = get_model(state_dict, args.arch).cuda()

    evaluator = ssl_eval.Evaluator(model,
                                   args.dataset,
                                   args.data_root,
                                   n_views=args.n_views,
                                   batch_size=args.batch_size)

    torch.backends.cudnn.benchmark = True
    embs = evaluator.generate_embeddings()

    if "linear_eval" in args.method:
        lr = args.batch_size / 256 * 0.1
        acc = evaluator.linear_eval(embs, batch_size=args.batch_size, lr=lr)
        if rank == 0:
            results['linear_eval'] = acc

    if "knn" in args.method:
        accs = evaluator.knn(embs, k=[1, 5, 20])
        if rank == 0:
            results['knn'] = {}
            results['knn'][1] = accs[0]
            results['knn'][5] = accs[5]
            results['knn'][20] = accs[20]

    if "snn" in args.method:
        acc = evaluator.snn(embs)
        if rank == 0:
            results['snn'] = acc

    return embs, results


def set_random_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    cudnn.deterministic = True
    print('WARNING: You have chosen to seed training. '
          'This will turn on the CUDNN deterministic setting, '
          'which can slow down your training considerably! '
          'You may see unexpected behavior when restarting '
          'from checkpoints.')


def get_model(state_dict, arch='resnet50'):
    model = models.__dict__[arch]()
    longest_module_name = ""
    for name, param in model.named_parameters():
        if name not in ['fc.weight', 'fc.bias']:
            param.requires_grad = False
        if len(name) > len(longest_module_name):
            longest_module_name = name
    model.fc = torch.nn.Identity()

    # If any prefixes preceed keys, remove those
    state_dict, prefix = _find_item_and_prefix(state_dict, longest_module_name)
    if len(prefix):
        for k in list(state_dict.keys()):
            if k.startswith(prefix):
                state_dict[k[len(prefix):]] = state_dict[k]

    return model


def _find_item_and_prefix(d, lookup_key):

    queue = [d]
    found = False

    while queue and not found:
        current_dict = queue.pop(0)
        for key, child in current_dict.items():
            if key.endswith(lookup_key):
                state_dict = current_dict
                prefix = key[:len(lookup_key)]
                found = True
                break
            elif isinstance(child, dict):
                queue.append(child)

    if not found:
        raise ValueError(f"{lookup_key} not found anywhere with any prefixes in dictionary.")

    return state_dict, prefix


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run inference on Imagenet, CIFAR10 or CIFAR100')
    parser.add_argument('model_path', type=str, help='Path to your model')
    parser.add_argument('data_root', type=str, help='Path to your data')
    parser.add_argument(
        '-d',
        '--dataset',
        type=str,
        default='imagenet',
        choices=['imagenet', 'cifar10', 'cifar100'],
        help='Name of your dataset, see options.',
    )
    parser.add_argument('-a',
                        '--arch',
                        type=str,
                        default='resnet50',
                        help="Torchvision architecture your model use as backend")
    parser.add_argument(
        '-n',
        '--n-views',
        type=int,
        default=3,
        help=
        "Number of views you want to generate from each sample. " + \
        "Note that the more sample you make, the more RAM you need."
    )
    parser.add_argument('-b',
                        '--batch-size',
                        type=int,
                        default=4096,
                        help="Batch size. We recommend 512 per each 10GB GPU-RAM you have.")
    parser.add_argument('-s', '--seed', type=int, default=None, help="Seed of run.")
    parser.add_argument('--devices',
                        type=str,
                        nargs='+',
                        default=['0'],
                        help="CUDA devices to use.")
    parser.add_argument('--method',
                        type=str,
                        nargs='+',
                        default=['linear_eval', "knn", "snn"],
                        choices=['linear_eval', "knn", "snn"],
                        help="Methods to you wish to run.")
    args = parser.parse_args()

    run_multi_gpu(args)
